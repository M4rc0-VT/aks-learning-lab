Welcome back\! üèóÔ∏è It is Friday night‚Äîthe perfect time for some low-stress infrastructure planning before the weekend hits full swing.

Since you are back, I assume the network is holding up (hotspot or otherwise) and little Benjamin might be settling down.

We are entering **Phase 2** of your roadmap.
**Phase 1 (Completed):** The "Inner Loop" (Docker $\rightarrow$ K8s $\rightarrow$ Helm). You know how to deploy apps.
**Phase 2 (Starting Now):** The "Outer Loop" (Infrastructure as Code). You will learn how to build the datacenter itself automatically.

### **Day 5: Infrastructure as Code (Terraform)**

We are going to stop clicking buttons in the Azure Portal.
Why? Because clicking is slow, error-prone, and unrepeatable.
Instead, we will write a recipe (Terraform code) that says: *"Azure, I want one Resource Group and one AKS cluster, please."*

#### **Step 1: The Tool Check** üß∞

Do you have Terraform installed on your laptop?

Run this in your terminal:

```powershell
terraform version
```

  * **If you see a version:** We are ready to rock.
  * **If it fails:** Run this command to install it:
    ```powershell
    winget install Hashicorp.Terraform
    ```
    *(You might need to restart your terminal after installing).*

#### **Step 2: The "Why" (Mental Model)**

While that installs/checks, here is the concept.

Currently, if I asked you to "Destroy your lab and rebuild it exactly the same way," you would have to remember:

1.  *Did I use B2s or D2as\_v5?*
2.  *Was it East US or North Central US?*
3.  *Did I select Azure CNI Overlay or Kubenet?*

With Terraform, you create a file called `main.tf`. You run `terraform apply`.
If you delete everything, you just run `terraform apply` again, and **10 minutes later**, your exact environment is back. This is how enterprise Platform Engineers sleep at night.

-------------------------------------

### **Step 1: The Setup** üìÇ

We need a clean workspace. We shouldn't mix your Terraform code with your Helm charts.

1.  Open your terminal.
2.  Navigate to your lab folder: `cd C:\k8s-lab`
3.  Create a new directory for infrastructure: `mkdir terraform`
4.  Go inside: `cd terraform`

### **Step 2: The Provider (The Handshake)** ü§ù

Terraform is a universal tool (it works for AWS, Google, Azure, etc.). We have to explicitly tell it: *"Hey, please download the plugin to talk to **Azure**."*

1.  Create a file named **`main.tf`** inside that folder.
2.  Paste this "Boilerplate" code into it:

<!-- end list -->

```hcl
# 1. We tell Terraform which providers we need
terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0" # This grabs the latest stable version of the Azure plugin
    }
  }
}

# 2. We configure the Azure Provider
provider "azurerm" {
  features {} # This empty block is required by the plugin to work
}

# 3. Our First Resource: A Resource Group
resource "azurerm_resource_group" "aks_rg" {
  name     = "rg-terraform-lab"
  location = "North Central US" # Matches your manual region
}
```

**Save the file.**

### **Step 3: The Initialization** ‚öôÔ∏è

This is the first command you run in any new Terraform project. It looks at your `main.tf`, sees you need `azurerm`, and downloads the plugin to a hidden `.terraform` folder.

Run this in your terminal:

```powershell
terraform init
```
---------------------------


Now, let's write the code to define your AKS cluster. We are going to replicate the exact "Expert" architecture we built manually (Overlay Networking, Calico, Cost-Optimized Node), but this time defined in code.

### **Step 1: The Blueprint (main.tf)** üìù

Open your `main.tf` file again. Keep the existing lines (provider and resource group) and **append** this block to the bottom of the file.

I have added comments explaining *why* we chose each setting.

```hcl
# 4. The AKS Cluster Resource
resource "azurerm_kubernetes_cluster" "aks" {
  name                = "aks-terraform-lab"
  location            = azurerm_resource_group.aks_rg.location
  resource_group_name = azurerm_resource_group.aks_rg.name
  dns_prefix          = "aks-lab"

  # The "Cost Conscious" Node Pool
  default_node_pool {
    name       = "default"
    node_count = 1                     # Manual Scale (1 Node)
    vm_size    = "Standard_D2as_v5"    # The economical choice we found
  }

  # Identity: We let Azure manage the credentials (no passwords needed)
  identity {
    type = "SystemAssigned"
  }

  # The "Expert" Network Profile
  network_profile {
    network_plugin      = "azure"      # Use Azure CNI
    network_plugin_mode = "overlay"    # The "Overlay" mode (Saves IPs!)
    network_policy      = "calico"     # Security Policy engine
    pod_cidr            = "10.244.0.0/16"
    service_cidr        = "10.0.0.0/16"
    dns_service_ip      = "10.0.0.10"
  }
}
```

**Save the file.**

-----

### **Step 2: The Dry Run (`plan`)** üïµÔ∏è‚Äç‚ôÇÔ∏è

Before we spend any money, we ask Terraform: *"What would happen if I pressed the button?"*

Run this command:

```powershell
terraform plan
```

**What to expect:**

  * It might ask you to login. If so, it will use your existing `az login` session.
  * It will output a long list of `+` (green plus signs).
  * **Look at the bottom:** It should say `Plan: 2 to add, 0 to change, 0 to destroy.`

**(Note:** If you see an error about `network_plugin_mode`, run `terraform init -upgrade` to ensure you have the absolute newest Azure plugin that supports the Overlay feature.)

**Does the Plan show "2 to add"?**

-----

### **Step 3: The Execution (`apply`)** üöÄ

If the plan looks good, let's build the datacenter.

```powershell
terraform apply
```

  * It will show the plan again and pause.
  * Type **`yes`** and hit Enter.

**Warning:** This will take about **4 to 8 minutes** (Azure has to physically create the VM and Cluster).

-------------------------

No, the **State File** is **NOT** the `main.tf` file. This is the single most common confusion for beginners! üõë

Let's clear this up while your cluster builds.

### **The Blueprint vs. The Diary**



1.  **`main.tf` (The Blueprint):**
    * This is the file you just wrote.
    * It describes **What you WANT** (e.g., "I want 1 Cluster").
    * It is safe to share with your team.

2.  **`terraform.tfstate` (The Diary/State File):**
    * This file is **created automatically** by Terraform the moment `terraform apply` finishes.
    * It records **What actually EXISTS** (e.g., "I created Cluster ID `aks-123` with IP `10.0.0.5`").
    * **The Danger:** It often contains **sensitive data** (passwords, keys) in plain text. You should **never** share this file casually or commit it to a public GitHub repo.

### **Why is it "Dangerous"?** üí£

If you accidentally delete `main.tf`, it's annoying, but you can rewrite it.
If you delete `terraform.tfstate`, **Terraform gets amnesia.**

* **Scenario:** You have a running cluster. You delete the state file.
* **Result:** You run `terraform apply`. Terraform looks at its (empty) memory and says: *"I don't remember building anything. I see you want a cluster. Let me try to build a NEW one."*
* **The Crash:** It tries to create `rg-terraform-lab`, but Azure screams: *"Error! That Resource Group already exists!"*

### **The Expert Move: Check it yourself** üïµÔ∏è‚Äç‚ôÇÔ∏è

Your `terraform apply` should be finishing up (or close to it).

1.  Open your **File Explorer** to the `C:\k8s-lab\terraform` folder.
2.  Do you see a new file named **`terraform.tfstate`**?

**Open it with Notepad.**
You will see a huge JSON block. That is the "Brain" of Terraform. It maps your code (`resource "azurerm_kubernetes_cluster"`) to the real world (`/subscriptions/c231.../resourceGroups/...`).

--------------------------

While `terraform apply` does its heavy lifting (it usually takes 5-8 minutes), let's decode exactly what you just wrote.

You effectively wrote a "Contract" with Azure. You told it *what* you want, but not *how* to build it. Terraform figures out the "how."

### **1. The Reference "Bible"** üìñ

You asked where to find all the options. There is one single source of truth for every Terraform engineer: **The Terraform Registry**.

  * **For the Cluster:** [Docs: azurerm\_kubernetes\_cluster](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/kubernetes_cluster)
      * *This page lists every single knob you can turn (like enabling monitoring, setting auto-scaling, attaching disks, etc.).*
  * **For the Resource Group:** [Docs: azurerm\_resource\_group](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/resource_group)

### **2. Decoding Your `main.tf`** üîç

Here is line-by-line where I got those values and why we picked them based on our "Manual" days.

#### **Block A: The Resource Group**

```hcl
resource "azurerm_resource_group" "aks_rg" {
  name     = "rg-terraform-lab"
  location = "North Central US"
}
```

  * **`resource "type" "name"`**: This is standard Terraform syntax.
      * `azurerm_resource_group` is the **Type** (defined by Azure).
      * `aks_rg` is the **Internal Name** (only Terraform knows this). You use this to reference it later (like `azurerm_resource_group.aks_rg.location`).
  * **`location`**: We chose **"North Central US"** because, during our manual test, "East US" failed with a quota error for the free tier. I hardcoded this to keep you safe from that error.

#### **Block B: The Cluster (The Big One)**

```hcl
resource "azurerm_kubernetes_cluster" "aks" {
  ...
  default_node_pool {
    name       = "default"
    node_count = 1
    vm_size    = "Standard_D2as_v5"
  }
```

  * **`vm_size`**: I pulled this directly from our "FinOps" investigation on Day 1.
      * Remember when we tried `B2s` and it failed?
      * Remember when we switched to `D2as_v5` (AMD) to get a cheaper price than Intel?
      * **That decision is now codified here.** If you ever want to upgrade to a bigger server, you just change this string to `"Standard_D4as_v5"` and run `apply`. Azure will swap the hardware for you automatically.

#### **Block C: The Networking (The "Expert" Configuration)**

```hcl
  network_profile {
    network_plugin      = "azure"
    network_plugin_mode = "overlay"  # <--- The Modern Standard
    network_policy      = "calico"   # <--- The Security Standard
    pod_cidr            = "10.244.0.0/16"
  }
```

  * **`network_plugin_mode = "overlay"`**: This is the critical setting we discussed on Day 1.
      * **Without this:** Every Pod takes an IP from your corporate network (IP Exhaustion risk).
      * **With this:** Pods live in a private bubble (`10.244.0.0/16`) and don't steal real IPs.
  * **`network_policy = "calico"`**: This installs the security engine so we can write firewall rules later.

### **3. Has it finished?** ‚è≥

Check your terminal.

  * **Success:** You should see green text: `Apply complete! Resources: 2 added...`
  * **Next Step:** If it is done, we need to get the credentials (keys) for this *new* cluster, because your `kubectl` is still trying to talk to the *old* manual one (which is deleted).

Run this command to switch your laptop to the new Terraform cluster:

```powershell
az aks get-credentials --resource-group rg-terraform-lab --name aks-terraform-lab --overwrite-existing
```

**(Then run `kubectl get nodes` to confirm you are online\!)**

---------------------

That is the "Brain" of Terraform you are looking at. üß†

Since you have it open in VS Code, let's do a quick "Security Audit" so you understand why this file is so dangerous.

### **1. The Security Audit** üïµÔ∏è‚Äç‚ôÇÔ∏è

Press **Ctrl+F** inside that `terraform.tfstate` file and search for this specific word:

`"kube_config"`

Do you see a massive block of random characters (like `YXBpVmVyc2lvbjogdjE...`) next to it?

  * **What is that?** That is the Base64-encoded administrative password for your cluster.
  * **The Risk:** If you were to commit this file to a public GitHub repository, a bot would find it in seconds, decode it, and hijack your cluster to mine bitcoin.
  * **The Rule:** Always add `*.tfstate` and `*.tfstate.backup` to your `.gitignore` file.

-----

### **2. The Final Exam: Does it work?** üìù

You built this datacenter with code. Now, let's prove it works by deploying your Helm chart from Day 4.

Since this is a brand new cluster, we need to link your tools to it first.

**Step 1: Update your credentials**
(Run this if you haven't already)

```powershell
az aks get-credentials --resource-group rg-terraform-lab --name aks-terraform-lab --overwrite-existing
```

**Step 2: Deploy your Chart**
Navigate back to your lab folder and install the app.

```powershell
cd C:\k8s-lab
helm install terraform-test ./my-nginx-app
```

**Step 3: Verify**

```powershell
kubectl get pods
```

**If you see 3 pods running, you have officially achieved "Cloud Native Automation."**

1.  **Infrastructure:** Managed by Terraform.
2.  **Application:** Managed by Helm.
3.  **Manual Steps:** Zero.

-----

### **3. The "FinOps" Cleanup (The Weekender)** üßπ

Since it is the weekend, we don't want to just "Stop" the cluster; we want to **Destroy** it.

  * **Why Destroy?** Terraform makes it so easy to rebuild (just run `apply` again on Monday) that we might as well save the disk storage costs too.
  * **The Command:**

<!-- end list -->

```powershell
cd C:\k8s-lab\terraform
terraform destroy
```

  * It will show you a plan with many red `-` signs (Resources to delete).
  * Type **`yes`**.

**This will strictly delete every resource defined in your `main.tf`.** No dangling Load Balancers, no hidden disks. A perfectly clean slate.

**Congratulations on completing Week 1\!** üèÜ
You went from "Manual ClickOps" to "Full Infrastructure as Code."
Enjoy your weekend with Benjamin\! üë∂ Do you want me to verify anything else before you shut down?
