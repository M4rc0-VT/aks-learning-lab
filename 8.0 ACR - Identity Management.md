### **Recaptulating** 

* **Last time (Day 5):** You built a **Private Vault (ACR)** and pushed your Nginx image to it.
* **The Problem:** Your **Cluster (AKS)** is currently locked out of that vault. If you tried to deploy that image now, Kubernetes would say: *"ErrImagePull: Access Denied."*
* **Today's Mission (Day 7):** We are going to give your Cluster the "Keys" to the Vault using **Terraform**.

-----

### **Step 1: The "Digital Handshake" (IAM)** ü§ù

In the old days, we would create a username/password for the cluster. That is bad security (passwords get leaked).
The **Modern Way** is to use **Azure IAM (Identity Access Management)**.

We need to tell Azure: *"Hey, this specific Cluster is allowed to `Pull` images from this specific Registry."*

**Action:**

1.  Open your **`C:\k8s-lab\terraform\main.tf`** file.
2.  Scroll to the very bottom.
3.  Add this new block. It is the "Bridge" connecting your two resources:

<!-- end list -->

```hcl
# 7. Grant AKS permission to Pull from ACR
resource "azurerm_role_assignment" "aks_to_acr" {
  # The "Who": The Kubelet (the agent on the node that actually pulls images)
  principal_id                     = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id

  # The "What": The "AcrPull" role (Read-only access)
  role_definition_name             = "AcrPull"

  # The "Where": Your specific registry
  scope                            = azurerm_container_registry.acr.id

  # Skip checks to make it faster for labs
  skip_service_principal_aad_check = true
}
```

*Note: Make sure you removed that broken "Clean Up Task" block from last time if you haven't already\!*

-----

### **Step 2: The Resurrection** üèóÔ∏è

Since you (hopefully) ran `terraform destroy` before your trip, your data center is currently gone. We need to rebuild it with this new permission included.

1.  Open your terminal to `C:\k8s-lab\terraform`.
2.  Run the builder command:
    ```powershell
    terraform apply
    ```
    *(Type `yes` when asked).*

### **The "Two Identities" Problem** üé≠

When you create an AKS cluster, Azure actually creates **two** different Digital IDs for it. It's like a company having an "Office Manager" and a "Warehouse Worker."

#### **1. The Cluster Identity (The Office Manager)**

  * **What it is:** The identity of the Control Plane (the API Server).
  * **What it does:** It manages Azure resources. When you create a Service with `LoadBalancer`, this identity talks to Azure and says: *"Hey, please bill me for a Public IP."*
  * **In Terraform:** `azurerm_kubernetes_cluster.aks.identity`

#### **2. The Kubelet Identity (The Warehouse Worker)** üì¶

  * **What it is:** The identity of the "Kubelet" agent that runs on every single Node (VM).
  * **What it does:** Its job is to look at a Pod definition and say: *"Okay, I need to fetch this box (Image) from the shelf."*
  * **Why we used it:** Since the **Kubelet** is the one actually running the `docker pull` command, **IT** is the one that needs the keys to the vault (ACR). If you gave the keys to the "Office Manager," the "Warehouse Worker" would still be locked out.

### **The Code Translation**

That is why your Terraform line looks like this:

```hcl
principal_id = azurerm_kubernetes_cluster.aks.kubelet_identity[0].object_id
```

We are explicitly telling Azure: *"Take the ID badge of the **Kubelet (Warehouse Worker)** and add it to the 'Allowed List' on the Registry."*

-----

**When it is done, don't forget the ritual:**
Since you destroyed the old cluster, your laptop's keys are invalid. You must run this to get the new keys:

```powershell
az aks get-credentials --resource-group rg-terraform-lab --name aks-terraform-lab --overwrite-existing
```

------------------
You have built the Digital Highway between your Cluster and your Registry! üõ£Ô∏è

Now comes the "Reality Check." Because you ran `terraform destroy` last lesson, **your new Registry is empty.** (New Resource = New Empty Storage).

We need to refill it and then tell Kubernetes to use it.

### **Step 1: Refill the Vault** üì¶

1.  **Find your new Registry Name:**
    Go to the Azure Portal or run this command to see the new random name Terraform generated:

    ```powershell
    az acr list --resource-group rg-terraform-lab --query "[].name" --output tsv
    ```

    *(Copy that name. It will be `acrlearninglab...`)*

2.  **Login & Push:**
    Run these commands to put your image back:

    ```powershell
    # 1. Login to the new registry
    az acr login --name <YOUR_NEW_ACR_NAME>

    # 2. Tag the image (Point it to the new address)
    docker tag nginx:latest <YOUR_NEW_ACR_NAME>.azurecr.io/my-nginx:v1

    # 3. Push it
    docker push <YOUR_NEW_ACR_NAME>.azurecr.io/my-nginx:v1
    ```

-----

### **Step 2: Update Your Helm Chart** ‚õëÔ∏è

1.  Open your **`values.yaml`** file (inside `C:\k8s-lab\my-nginx-app`).
2.  Update it to look like this:
```
replicaCount: 3
acrDnsName: acrlearninglabvi3zh.azurecr.io
imageName: my-nginx
imageVersion: "v1"
```
3.  Open your template files (**`nginx-quickstart-deployment.yaml`**, **`nginx-quickstart-service.yaml`**, **`nginx-quickstart-ingress.yaml`**)
4. Replace all the nginx references with **`{{ .Values.imageName }}`**

-----

### **Step 3: The Moment of Truth** üöÄ

Deploy the chart. If the permissions are working, Kubernetes will pull the image instantly.

```powershell
# Install (or Upgrade)
helm upgrade --install my-release ./my-nginx-app
```

**Check the status:**

```powershell
kubectl get pods
```

**What we want to see:** `Running`.
**What we fear:** `ImagePullBackOff` or `ErrImagePull`.

---------------

### **The "Hidden Trap" Check** üïµÔ∏è‚Äç‚ôÇÔ∏è
The pods are **Running**, which is the victory for the **Identity/ACR** lesson.

**However**, since you ran `terraform destroy` over the weekend, you wiped out everything, including the **Nginx Ingress Controller** (the "Doorbell" software).

Your new `nginx-quickstart-ingress.yaml` file is valid code, but right now, **it is talking to a wall.** There is no Controller listening for that `ingressClassName: nginx`.

**The Fix:**
Re-install the Ingress Controller "Software" onto your fresh Terraform "Hardware":

```powershell
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm repo update

# Install the Controller (The Doorbell)
helm install ingress-nginx ingress-nginx/ingress-nginx `
  --namespace ingress-basic --create-namespace `
  --set controller.service.annotations."service\.beta\.kubernetes\.io/azure-load-balancer-health-probe-request-path"=/healthz
```

**Once that finishes:**

1.  Get your new Public IP: `kubectl get services -n ingress-basic`
2.  Visit that IP in Chrome.
3.  **If you see "Welcome to Nginx", you have fully restored the lab.**

-----------------------

You shouldn't have to manually type `helm install` every time you rebuild the lab. That command belongs in your Terraform code. This concept is called **"Bootstrapping the Cluster."**

We are going to tell Terraform: *"After you build the hardware (AKS), please install the essential software (Ingress Controller) immediately."*

### **Step 1: The "Helm" Provider** üîå

Just like we had to tell Terraform how to speak "Azure," we now need to tell it how to speak "Helm."

Open your `main.tf` and add the `helm` provider to the `required_providers` block at the top, and then configure it to talk to your **new** cluster.

*(Replace your top `terraform` block and add the provider block below it)*:

```hcl
terraform {
  required_providers {
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
    # 1. Add the Helm Provider here
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.0"
    }
  }
}

provider "azurerm" {
  features {}
}

# 2. Configure Helm to talk to the JUST CREATED cluster
# (We don't use a file; we use the credentials directly from the resource!)
provider "helm" {
  kubernetes {
    host                   = azurerm_kubernetes_cluster.aks.kube_config.0.host
    client_certificate     = base64decode(azurerm_kubernetes_cluster.aks.kube_config.0.client_certificate)
    client_key             = base64decode(azurerm_kubernetes_cluster.aks.kube_config.0.client_key)
    cluster_ca_certificate = base64decode(azurerm_kubernetes_cluster.aks.kube_config.0.cluster_ca_certificate)
  }
}
```

-----

### **Step 2: The Ingress Resource** üèóÔ∏è

Now that Terraform can speak Helm, let's add the Ingress Controller as a resource at the bottom of your file.

```hcl
# 8. Install the Ingress Controller (The Doorbell) automatically
resource "helm_release" "ingress_nginx" {
  name             = "ingress-nginx"
  repository       = "https://kubernetes.github.io/ingress-nginx"
  chart            = "ingress-nginx"
  namespace        = "ingress-basic"
  create_namespace = true

  # Start it only AFTER the cluster is ready
  depends_on = [azurerm_role_assignment.aks_to_acr]

  # Set the specific health probe value we learned about manually
  set {
    name  = "controller.service.annotations.service\\.beta\\.kubernetes\\.io/azure-load-balancer-health-probe-request-path"
    value = "/healthz"
  }
}
```

### **Why this is better**

Next time you run `terraform apply`:

1.  Terraform builds the Cluster.
2.  Terraform grants the Permissions (Identity).
3.  Terraform **automatically** downloads and installs the Ingress Controller.
4.  You can go straight to deploying your app.

**Action:** Add these blocks to your `main.tf`, run `terraform init` (to download the new Helm plugin), and then `terraform apply`.

You have just built a **Self-Service Platform**. üöÄ